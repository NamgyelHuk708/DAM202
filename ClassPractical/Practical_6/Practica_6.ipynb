{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36583799",
      "metadata": {
        "id": "36583799"
      },
      "source": [
        "# Practical 6: Transformer Architecture Implementation (PyTorch)\n",
        "\n",
        "**Course:** DAM202 [Year3-Sem1]\n",
        "\n",
        "**Focus:** Implementation of the original \"Attention Is All You Need\" (Vaswani et al., 2017) Transformer architecture from scratch using PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d1f41f1",
      "metadata": {
        "id": "5d1f41f1"
      },
      "source": [
        "## Introduction and Setup\n",
        "\n",
        "This notebook implements the Transformer architecture from scratch. We will build all the core components, including Multi-Head Attention, Positional Encoding, and the full Encoder-Decoder stack.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e6707a72",
      "metadata": {
        "id": "e6707a72"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f809a0",
      "metadata": {
        "id": "32f809a0"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "These are the standard hyperparameters for the base Transformer model, as defined in the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aa292d85",
      "metadata": {
        "id": "aa292d85"
      },
      "outputs": [],
      "source": [
        "# Base Model Hyperparameters\n",
        "d_model = 512  # The dimension of the embeddings and sub-layer outputs\n",
        "N = 6          # The number of layers in the encoder and decoder\n",
        "h = 8          # The number of attention heads\n",
        "d_k = 64       # The dimension of the key and query vectors (d_model / h)\n",
        "d_v = 64       # The dimension of the value vectors (d_model / h)\n",
        "d_ff = 2048    # The inner dimension of the position-wise feed-forward network\n",
        "dropout = 0.1  # The dropout rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca013a2",
      "metadata": {
        "id": "4ca013a2"
      },
      "source": [
        "## Part 1: Core Components\n",
        "\n",
        "We start by implementing the fundamental building blocks of the Transformer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55405da",
      "metadata": {
        "id": "a55405da"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "This is the core attention mechanism. It computes the dot products of the query with all keys, divides each by the square root of the key dimension, and applies a softmax function to obtain the weights on the values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "aa9a8917",
      "metadata": {
        "id": "aa9a8917"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Calculate the scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        q (torch.Tensor): Query tensor; shape (batch_size, n_heads, seq_len_q, d_k)\n",
        "        k (torch.Tensor): Key tensor; shape (batch_size, n_heads, seq_len_k, d_k)\n",
        "        v (torch.Tensor): Value tensor; shape (batch_size, n_heads, seq_len_v, d_v)\n",
        "        mask (torch.Tensor, optional): Mask tensor. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The output of the attention mechanism.\n",
        "        torch.Tensor: The attention weights.\n",
        "    \"\"\"\n",
        "    # MatMul Q and K^T\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    # Apply mask (if provided)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Softmax to get attention weights\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # MatMul with V\n",
        "    output = torch.matmul(attn_weights, v)\n",
        "    return output, attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc36d015",
      "metadata": {
        "id": "bc36d015"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions. It runs the scaled dot-product attention mechanism in parallel multiple times.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e8abb06",
      "metadata": {
        "id": "7e8abb06"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.d_k = d_model // h\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        return x.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # Linear projections\n",
        "        q = self.W_q(q)\n",
        "        k = self.W_k(k)\n",
        "        v = self.W_v(v)\n",
        "\n",
        "        # Split into h heads\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_output, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # Concatenate heads\n",
        "        batch_size, _, seq_len, _ = attn_output.size()\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.W_o(attn_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ef726f",
      "metadata": {
        "id": "07ef726f"
      },
      "source": [
        "### Position-wise Feed-Forward Network\n",
        "\n",
        "This is a fully connected feed-forward network applied to each position separately and identically. It consists of two linear transformations with a ReLU activation in between.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67196afc",
      "metadata": {
        "id": "67196afc"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db401bf6",
      "metadata": {
        "id": "db401bf6"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "Since the model contains no recurrence or convolution, we inject information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3a281d47",
      "metadata": {
        "id": "3a281d47"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11eaa38c",
      "metadata": {
        "id": "11eaa38c"
      },
      "source": [
        "## Part 2: Building the Layers\n",
        "\n",
        "Now we combine the core components to build the Encoder and Decoder layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7db500c",
      "metadata": {
        "id": "e7db500c"
      },
      "source": [
        "### Encoder Layer\n",
        "\n",
        "Each encoder layer has two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6b7b46a0",
      "metadata": {
        "id": "6b7b46a0"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, h, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, h)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Self-attention sub-layer\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed-forward sub-layer\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e7aa8ef",
      "metadata": {
        "id": "5e7aa8ef"
      },
      "source": [
        "### Decoder Layer\n",
        "\n",
        "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a363a8b9",
      "metadata": {
        "id": "a363a8b9"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, h, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, h)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, h)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        # Masked self-attention sub-layer\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Cross-attention sub-layer\n",
        "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        # Feed-forward sub-layer\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a888bc3e",
      "metadata": {
        "id": "a888bc3e"
      },
      "source": [
        "## Part 3: Assembling the Full Transformer\n",
        "\n",
        "We now assemble the full Transformer model by stacking the encoder and decoder layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d1de377a",
      "metadata": {
        "id": "d1de377a"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, h, d_ff, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, h, d_ff, dropout) for _ in range(N)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, h, d_ff, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, h, d_ff, dropout) for _ in range(N)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c480e1e",
      "metadata": {
        "id": "3c480e1e"
      },
      "source": [
        "### The Transformer Model\n",
        "\n",
        "This is the final model, which combines the Encoder, Decoder, and a final linear layer to produce the output probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "615ee57c",
      "metadata": {
        "id": "615ee57c"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, h, d_ff, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, N, h, d_ff, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, N, h, d_ff, dropout)\n",
        "        self.final_linear = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def create_padding_mask(self, seq, pad_token=0):\n",
        "        # (batch_size, 1, 1, seq_len)\n",
        "        return (seq != pad_token).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    def create_look_ahead_mask(self, size):\n",
        "        mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool)\n",
        "        return mask == 0\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_pad_mask = self.create_padding_mask(src)\n",
        "        tgt_pad_mask = self.create_padding_mask(tgt)\n",
        "\n",
        "        look_ahead_mask = self.create_look_ahead_mask(tgt.size(1))\n",
        "\n",
        "        # Combine padding mask and look-ahead mask for the target\n",
        "        tgt_mask = tgt_pad_mask & look_ahead_mask\n",
        "\n",
        "        encoder_output = self.encoder(src, src_pad_mask)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, src_pad_mask, tgt_mask)\n",
        "\n",
        "        output = self.final_linear(decoder_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c984a1f9",
      "metadata": {
        "id": "c984a1f9"
      },
      "source": [
        "## Part 4: Basic Functionality Test\n",
        "\n",
        "We'll now instantiate the model and perform a forward pass with dummy data to ensure all components are connected correctly and the tensor dimensions are valid.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "658afe26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "658afe26",
        "outputId": "9a1ca496-43de-4f50-a810-b132b342a544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 12, 5000])\n"
          ]
        }
      ],
      "source": [
        "# Vocabulary sizes for source and target languages\n",
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "\n",
        "# Instantiate the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, h, d_ff, dropout)\n",
        "\n",
        "# Create dummy input tensors\n",
        "# (batch_size, seq_len)\n",
        "src_seq = torch.randint(1, src_vocab_size, (2, 10))  # Batch of 2, sequence length 10\n",
        "tgt_seq = torch.randint(1, tgt_vocab_size, (2, 12))  # Batch of 2, sequence length 12\n",
        "\n",
        "# Perform a forward pass\n",
        "output = model(src_seq, tgt_seq)\n",
        "\n",
        "# Print the output shape to verify\n",
        "print(\"Output shape:\", output.shape)\n",
        "# Expected output shape: (batch_size, tgt_seq_len, tgt_vocab_size) -> (2, 12, 5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "63eb5dbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63eb5dbf",
        "outputId": "01ff6731-5c05-4453-a6a9-1e200c9fa5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentence: [10 25  5 30 42]\n",
            "--- Starting Greedy Decoding ---\n",
            "Step 1: Predicted token index = 1626\n",
            "Step 2: Predicted token index = 714\n",
            "Step 3: Predicted token index = 2405\n",
            "Step 4: Predicted token index = 1474\n",
            "Step 5: Predicted token index = 4229\n",
            "Step 6: Predicted token index = 4682\n",
            "Step 7: Predicted token index = 2247\n",
            "Step 8: Predicted token index = 1810\n",
            "Step 9: Predicted token index = 3137\n",
            "Step 10: Predicted token index = 1931\n",
            "Step 11: Predicted token index = 1731\n",
            "Step 12: Predicted token index = 4505\n",
            "Step 13: Predicted token index = 1796\n",
            "Step 14: Predicted token index = 4770\n",
            "Step 15: Predicted token index = 2113\n",
            "\n",
            "--- Final Generated Sequence ---\n",
            "[   1 1626  714 2405 1474 4229 4682 2247 1810 3137 1931 1731 4505 1796\n",
            " 4770 2113]\n"
          ]
        }
      ],
      "source": [
        "# Let's create a hypothetical scenario for inference\n",
        "# Assume we have a trained model (ours is randomly initialized, but the process is the same)\n",
        "\n",
        "# Define special tokens (hypothetical vocabulary indices)\n",
        "SRC_PAD_TOKEN = 0\n",
        "TGT_PAD_TOKEN = 0\n",
        "TGT_SOS_TOKEN = 1 # Start of Sequence\n",
        "TGT_EOS_TOKEN = 2 # End of Sequence\n",
        "\n",
        "# Create a dummy source sentence (e.g., batch of 1, sequence of 5)\n",
        "# In a real scenario, this would be tokenized text.\n",
        "src_sentence = torch.tensor([[10, 25, 5, 30, 42]]) # (1, 5)\n",
        "\n",
        "# The decoder starts with just the Start-Of-Sequence token\n",
        "tgt_sentence = torch.tensor([[TGT_SOS_TOKEN]]) # (1, 1)\n",
        "\n",
        "# Set a max length to prevent infinite loops\n",
        "max_output_len = 15\n",
        "\n",
        "print(f\"Source Sentence: {src_sentence.numpy()[0]}\")\n",
        "print(\"--- Starting Greedy Decoding ---\")\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad(): # We don't need to track gradients for inference\n",
        "    for i in range(max_output_len):\n",
        "        # Get the model's output\n",
        "        output = model(src_sentence, tgt_sentence) # Shape: (1, current_tgt_len, tgt_vocab_size)\n",
        "\n",
        "        # Get the probabilities for the very last token in the sequence\n",
        "        last_token_logits = output[:, -1, :] # Shape: (1, tgt_vocab_size)\n",
        "\n",
        "        # Find the token with the highest probability (greedy choice)\n",
        "        predicted_token = torch.argmax(last_token_logits, dim=-1) # Shape: (1)\n",
        "\n",
        "        # Append the predicted token to the target sentence\n",
        "        tgt_sentence = torch.cat([tgt_sentence, predicted_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        print(f\"Step {i+1}: Predicted token index = {predicted_token.item()}\")\n",
        "\n",
        "        # If the model predicts the End-Of-Sequence token, we stop\n",
        "        if predicted_token.item() == TGT_EOS_TOKEN:\n",
        "            print(\"--- End-of-Sequence token generated. Stopping. ---\")\n",
        "            break\n",
        "\n",
        "print(\"\\n--- Final Generated Sequence ---\")\n",
        "print(tgt_sentence.numpy()[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb07abe0",
      "metadata": {
        "id": "eb07abe0"
      },
      "source": [
        "## Part 5: Simple Inference Example (Greedy Decoding)\n",
        "\n",
        "The test above confirms our model's architecture is sound. But how would we use it to generate a sequence? Here is a simple example of a \"greedy decoding\" loop for inference.\n",
        "\n",
        "At each step, we take the token with the highest probability from the model's output and feed it back in as input for the next step. We continue this until the model produces an \"end-of-sequence\" token or we reach a maximum length.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab131c1",
      "metadata": {
        "id": "5ab131c1"
      },
      "source": [
        "## Part 5: Report and Documentation\n",
        "\n",
        "This section contains the written report as required by the practical guide.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63dc030a",
      "metadata": {
        "id": "63dc030a"
      },
      "source": [
        "### 1. Architectural Explanation\n",
        "\n",
        "**Overall Structure:**\n",
        "The Transformer model is composed of an **Encoder** and a **Decoder**. The Encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the Decoder then generates an output sequence $(y_1, ..., y_m)$ one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
        "\n",
        "**Attention Mechanisms:**\n",
        "- **Scaled Dot-Product Attention:** The core of the model. The output is a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. The scaling factor $\\frac{1}{\\sqrt{d_k}}$ prevents the dot products from growing too large.\n",
        "- **Multi-Head Attention:** Instead of performing a single attention function, we project the queries, keys, and values $h$ times with different, learned linear projections. Attention is performed in parallel on each of these projected versions. The results are concatenated and once again projected, resulting in the final values. This allows the model to jointly attend to information from different representation subspaces.\n",
        "\n",
        "**Positional Encoding and Masking:**\n",
        "- **Positional Encoding:** Since the model has no recurrence, we inject positional encodings to give the model information about the sequence order. These are sine and cosine functions of different frequencies.\n",
        "- **Padding Mask:** Used to ignore `<PAD>` tokens in the input sequences, ensuring they don't contribute to the attention calculation.\n",
        "- **Look-Ahead Mask:** Used in the decoder's self-attention to prevent positions from attending to subsequent positions. This ensures that the prediction for position $i$ can depend only on the known outputs at positions less than $i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c50f19",
      "metadata": {
        "id": "29c50f19"
      },
      "source": [
        "### 2. Code Structure and Design\n",
        "\n",
        "**Modular Design:**\n",
        "The code is structured in a modular, bottom-up fashion.\n",
        "- **Core Components (`MultiHeadAttention`, `PositionwiseFeedForward`, `PositionalEncoding`):** These are the fundamental building blocks, implemented as separate, reusable `nn.Module`s or functions. This separation makes the code cleaner and easier to debug.\n",
        "- **Layers (`EncoderLayer`, `DecoderLayer`):** These modules combine the core components to form a single layer of the encoder or decoder. This abstraction simplifies the final model construction.\n",
        "- **Full Model (`Transformer`):** The final `Transformer` class assembles the stacks of encoder and decoder layers and adds the necessary embedding and output layers.\n",
        "\n",
        "**Residual Connections and Layer Normalization:**\n",
        "Each sub-layer (self-attention, feed-forward) in the `EncoderLayer` and `DecoderLayer` is wrapped in a residual connection followed by layer normalization. This is implemented as `self.norm(x + self.dropout(sublayer(x)))`.\n",
        "- **Residual Connections:** Help prevent the vanishing gradient problem in deep networks, allowing gradients to flow more directly through the network.\n",
        "- **Layer Normalization:** Stabilizes the training process by normalizing the inputs to each sub-layer, leading to faster convergence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a888b86",
      "metadata": {
        "id": "9a888b86"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}